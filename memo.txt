NCCL test on Reedbush-H

使用moduleは以下の通り
module load cuda/8.0.44 intel/17.0.1.132 mvapich2-gdr/2.2/intel

nccl2を拾ってきて、./ncclとして見えるにように置く
(./nccl/includeと./nccl/lib64が見えるようにしておく）

SYS=rbh make reduce_nccl
(mpicc -O3 -qopenmp -xCORE-AVX2 -o reduce_nccl -lcudart -I/lustre/app/acc/cuda/8.0.44/include -L/lustre/app/acc/cuda/8.0.44/lib64 -I./nccl/include -L./nccl/lib -lnccl reduce_nccl.c に相当、cudaのディレクトリは参照しなくていいんじゃないかってのは置いておく)

qsub rbh_reduce_nccl.sh
プログラム中ではcudaSetDevice(0)している。
23行目のCUDA_VISIBLE_DEVICESで1プロセス1GPUに制限しており
cudaSetDevice(0)でノード内2プロセスがそれぞれ別のGPUを触りにいくはずだが、
これだとncclReduce/ncclAllReduceが死ぬ。
CUDA_VISIBLE_DEVICES設定を抜いて同じGPUを触らせると死なない。
CUDA_VISIBLE_DEVICES設定を抜いてcudasetDevice(myid)にして別のGPUを触らせると死ぬ。

NCCLの使い方が悪いのか、Reedbush-HのNVLINK周りがおかしいのか、どっちだろう？


NCCLテスト

1.
ジョブスクリプトでcuda_visible_devicesを指定しない
プログラム中でcudaSetDevice(myrank)により使うGPUを指定しない
-> プロセスごとに1GPUしか使えない、使うGPUが衝突しないとは限らない

2.
ジョブスクリプトでcuda_visible_devicesを指定しない
プログラム中でcudaSetDevice(myrank)により使うGPUを指定する
-> cudaSetDeviceでGPUを衝突させられる

3.
ジョブスクリプトでcuda_visible_devicesを指定する
プログラム中でcudaSetDevice(myrank)により使うGPUを指定しない
-> CUDA_VISIBLE_DEVICESの設定が正しければ使うGPUは衝突しない、プロセスごとに1GPUしか使えない

4.
ジョブスクリプトでcuda_visible_devicesを指定する
プログラム中でcudaSetDevice(myrank)により使うGPUを指定する
-> CUDA_VISIBLE_DEVICESの設定が正しければcudaSetDeviceでもGPUを衝突させられないはず、プロセスごとに複数のGPUを使える

CUDA_VISIBLE_DEVICES=0,1 + cudaSetDevice(0)
-> rbh_reduce_nccl.sh.o666045 | うごいた
CUDA_VISIBLE_DEVICES=0,1 + cudaSetDevice(myrank)
-> rbh_reduce_nccl.sh.o666058 | 動いた
CUDA_VISIBLE_DEVICES=rank + cudaSetDevice(0)
-> rbh_reduce_nccl.sh.o666051 | NG, ncclCommInitRank failed
CUDA_VISIBLE_DEVICES=rank + cudaSetDevice(myrank)
-> rbh_reduce_nccl.sh.o666055 | NG, 1 cudaSetDevice failed: invalid device ordinal

ジョブスクリプトから各MPIプロセスへは利用可能な全GPUのリストをCUDA_VISIBLE_DEVICESで引き渡す。
プロセスごとに分けない。
プログラム側で全GPUリストから実際に使いたいGPUを選べば良い。
